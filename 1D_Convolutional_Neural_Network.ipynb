{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1D Convolutional Neural Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOGvjs+Q1KBNMJSa1ik0aK5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ducbao811/diveintocode-ml/blob/master/1D_Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l83ih8uuPZ88"
      },
      "source": [
        "# [Problem 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epip47C3978X"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUEejsLXPReG"
      },
      "source": [
        "class ConvolutionalLayer:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "  \n",
        "  def forward(self, X):\n",
        "    self.X = X\n",
        "    filterSize = len(self.W)\n",
        "    A = np.empty(filterSize - 1)\n",
        "    for i in range(filterSize - 1):\n",
        "      A[i] = X[ i : i + filterSize] @ self.W + self.b\n",
        "    return A\n",
        "  \n",
        "  def backward(self, dA):\n",
        "    \n",
        "    \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The error receives from the later layer \n",
        "        Returns\n",
        "        ----------\n",
        "        dX : Gradient to flow forward\n",
        "    \"\"\"\n",
        "\n",
        "    self.dB = np.sum(dA, axis = 0)\n",
        "    self.dW = np.empty(len(self.W))\n",
        "\n",
        "    for i in range(len(self.dW)):\n",
        "      self.dW[i] = dA @ self.X[ i : i + len(dA) ]\n",
        "    \n",
        "    dX = np.empty(len(self.X))\n",
        "\n",
        "    for j in range(len(dX)):\n",
        "\n",
        "      for s in range(len(self.W)):\n",
        "        if s < j + 1 and s > j - len(dA):\n",
        "          dX[j] += dA[j-s] * self.W[s]    \n",
        "\n",
        "    return self.dW, self.dB, dX"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e-ioOr3f-nW"
      },
      "source": [
        "# [Problem 2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfJi3tbigKtd"
      },
      "source": [
        "def output_size_calculation(n_in, P, F, S):\n",
        "    return int((n_in + 2*P - F) / S + 1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucyUiSHgzJV5"
      },
      "source": [
        "# [Problem 3]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mVMJPhKP3Ly"
      },
      "source": [
        "x = np.array([1,2,3,4])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])\n",
        "delta_a = np.array([10, 20])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFrDs-6W9X51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16764db8-ed72-4993-bb4b-895d4d431a60"
      },
      "source": [
        "SCD1 = ConvolutionalLayer(w,b)\n",
        "a = SCD1.forward(x)\n",
        "print(a)\n",
        "db, dw, dx = SCD1.backward(delta_a)\n",
        "print(db)\n",
        "print(dw)\n",
        "print(dx)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[35. 50.]\n",
            "[ 50.  80. 110.]\n",
            "30\n",
            "[ 31. 111. 171. 140.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kra56n7rcIZz"
      },
      "source": [
        "# [Problem 4]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCUEWtwUcHcs"
      },
      "source": [
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1] \n",
        "    \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MemkpGtse4ya"
      },
      "source": [
        "class SimpleInitializer:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    \n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II3teUzCe666"
      },
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.b -= self.lr * layer.dB\n",
        "\n",
        "        return layer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ID8IZKBfLnk"
      },
      "source": [
        "class AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.delta = 1e-7\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Update weights and biases for a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : Instance of the layer before update\n",
        "        \"\"\"\n",
        "        # Update sum of squares of the gradient including the current iteration \n",
        "        layer.HW += layer.dW * layer.dW\n",
        "        layer.HB += layer.dB * layer.dB\n",
        "\n",
        "        # Update parameters\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.HW) + self.delta)\n",
        "        layer.b -= self.lr * layer.dB / (np.sqrt(layer.HB) + self.delta)\n",
        "\n",
        "        return layer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3bq-JCEp9vV"
      },
      "source": [
        "def output_size_calculation(n_in, F, P=0, S=1):\n",
        "    n_out = int((n_in + 2*P - F) / S + 1)\n",
        "    return n_out"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2yYnE3zfqPz"
      },
      "source": [
        "a one-dimensional convolutional layer class that does not limit the number of channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu_v6Q1nfQ27"
      },
      "source": [
        "class ConvolutionalLayer1D:\n",
        "    \n",
        "    def __init__(self, batch_size, optimizer=AdaGrad, initializer=SimpleInitializer, \n",
        "                 n_in_channels=1, n_out_channels=1, padding=0):\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_out_channels, n_in_channels, batch_size)\n",
        "        self.b = initializer.B(n_out_channels)\n",
        "        self.padding = padding\n",
        "        self.n_out_channels = n_out_channels\n",
        "        self.n_in_channels = n_in_channels\n",
        "        self.n_out = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.n_in = X.shape[-1] # number of features in input \n",
        "        # Calcualte number of features in output\n",
        "        self.n_out = output_size_calculation(n_in=self.n_in, F = self.batch_size, \n",
        "                                             P=self.padding)\n",
        "        X = X.reshape(self.n_in_channels, self.n_in)\n",
        "        self.X = np.pad(X, ((0,0), ((self.batch_size-1), 0))) # Add padding to input\n",
        "        self.X1 = np.zeros((self.n_in_channels, self.batch_size, self.n_in+(self.batch_size-1)))\n",
        "        for i in range(self.batch_size):\n",
        "            self.X1[:, i] = np.roll(self.X, -i, axis=-1)\n",
        "        A = np.sum(self.X1[:, :, self.batch_size-1-self.padding : self.n_in + self.padding] * \n",
        "                   self.W[:, :, :, np.newaxis], axis=(1, 2)) + self.b.reshape(-1,1)\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        self.dW = np.sum(np.dot(dA, self.X1[:, :, self.batch_size - 1 - self.padding : self.n_in+self.padding, np.newaxis]),\n",
        "                         axis=-1)\n",
        "        self.dB = np.sum(dA, axis=1)\n",
        "        self.dA = np.pad(dA, ((0,0), (0, (self.b_size-1))))\n",
        "        self.dA1 = np.zeros((self.n_out_channels, self.batch_size, self.dA.shape[-1]))\n",
        "        for i in range(self.b_size):\n",
        "            self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\n",
        "        dX = np.sum(self.W@self.dA1, axis=0)\n",
        "        self.optimizer.update(self)\n",
        "        return dX"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUeDbc11dn5h"
      },
      "source": [
        "test = ConvolutionalLayer1D(batch_size=3, initializer=SimpleInitializer(0.01), \n",
        "                            optimizer=SGD(0.01), n_in_channels=2, n_out_channels=3, padding=0)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mlk1lqbdq80"
      },
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
        "test.W = np.ones((3, 2, 3), dtype=float)\n",
        "test.B = np.array([1, 2, 3], dtype=float)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zULhnHa9d4yu",
        "outputId": "9a071c57-f525-4c91-81fa-2ec9bbb63bba"
      },
      "source": [
        "testing = test.forward(x)\n",
        "testing"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15.00914143, 21.00914143],\n",
              "       [14.98904867, 20.98904867],\n",
              "       [14.99111435, 20.99111435]])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXo_e9eBr77V"
      },
      "source": [
        "#[Problem 7]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJtP1LWBr-mu"
      },
      "source": [
        "class ConvolutionalLayer1D_Stride:\n",
        "    \n",
        "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\n",
        "        self.b_size = b_size\n",
        "        self.optimizer = optimizer\n",
        "        self.pa = pa\n",
        "        self.stride = stride\n",
        "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
        "        self.B = initializer.B(n_out_channels)\n",
        "        self.n_in_channels = n_in_channels\n",
        "        self.n_out_channels = n_out_channels\n",
        "        self.n_out = None\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.n_samples = X.shape[0]\n",
        "        self.n_in = X.shape[-1]\n",
        "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\n",
        "        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
        "        self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n",
        "        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
        "        for i in range(self.b_size):\n",
        "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
        "        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
        "        self.dB = np.sum(dA, axis=(0, -1))\n",
        "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
        "        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
        "        for i in range(self.b_size):\n",
        "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
        "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
        "        self.optimizer.update(self)\n",
        "        return dX"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSsnmHbzltIA"
      },
      "source": [
        "# [Problem 8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jTTPLuPmHaZ"
      },
      "source": [
        "Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXTpuwK4d6Nu"
      },
      "source": [
        "# Importing dataset\n",
        "\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshaping for fitting\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# Coverting value to 0 and 1\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "# Splitting data for training and validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_training, X_val, y_training, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "class Sigmoid:\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    \n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    \n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    \n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
        "\n",
        "class FC:\n",
        "\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X@self.W + self.B\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ\n",
        "\n",
        "class XavierInitializer:\n",
        "    \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "    \n",
        "class HeInitializer():\n",
        "    \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    \n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return\n",
        "\n",
        "class AdaGrad:\n",
        "    \n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "    \n",
        "    def update(self, layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-kQBW4AmkQt"
      },
      "source": [
        "import math\n",
        "class ScratchCNNClassifier():\n",
        "    \"\"\"\n",
        "    Simple three-layer Convolutional network classifier\n",
        "    Parameters\n",
        "    ----------\n",
        "    Attributes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose = False, bias = True, lr =0.01, n_features = 784,\n",
        "                 n_nodes1 = 400, n_nodes2 = 200, n_output = 10, sigma = 0.02, \n",
        "                 activate_func = Tanh, initializer = SimpleInitializer, optimizer=SGD):\n",
        "        self.verbose = verbose\n",
        "        self.has_bias = bias\n",
        "        self.lr = lr\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.activation_function = activate_func\n",
        "        self.batch_size = 20\n",
        "        self.epoch = 10\n",
        "        self.sigma = sigma\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn a neural network classifier.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : The following form of ndarray, shape (n_samples,)\n",
        "            Correct answer value of training data\n",
        "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            Features of verification data\n",
        "        y_val : The following form of ndarray, shape (n_samples,)\n",
        "            Correct value of verification data\n",
        "        \"\"\"\n",
        "        self.training_loss = []\n",
        "        self.testing_loss = []\n",
        "\n",
        "        # Initilize optimizer and layers\n",
        "        optimizer = self.optimizer(self.lr)\n",
        "        self.Con1D = ConvolutionalLayer1D_Stride(b_size=7, initializer=SimpleInitializer(0.01), \n",
        "                                              optimizer=self.optimizer(self.lr), \n",
        "                                              n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n",
        "        self.Con1D.n_out = output_size_calculation(X.shape[-1], self.Con1D.b_size, self.Con1D.pa,\n",
        "                                                   self.Con1D.stride)\n",
        "        self.activation1 = self.activation_function()\n",
        "        self.FC2 = FC(self.Con1D.n_out, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
        "        self.activation2 = self.activation_function()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializer(self.sigma), optimizer)\n",
        "        self.activation3 = Softmax()\n",
        "\n",
        "        for i in range (self.epoch):\n",
        "          \n",
        "          get_mini_batch = GetMiniBatch(X,y,batch_size=20)\n",
        "\n",
        "          for mini_X_train, mini_y_train in get_mini_batch:\n",
        "              self.forward_propagation(mini_X_train)\n",
        "              self.backward_propagation(mini_X_train, mini_y_train)\n",
        "\n",
        "          self.forward_propagation(X)\n",
        "          self.training_loss.append(self._cross_entropy(y,self.Z3))\n",
        "\n",
        "          if X_val is not None:\n",
        "            self.forward_propagation(X_val)\n",
        "            self.testing_loss.append(self._cross_entropy(y_val,self.Z3))\n",
        "          \n",
        "          if self.verbose:\n",
        "            print(\"Epoch {}:\\nTraining_loss: {}\".format(i,self.training_loss[-1]))\n",
        "            if X_val is not None:\n",
        "              print(\"Validation loss: {}\".format(self.testing_loss[-1]))\n",
        "\n",
        "    def forward_propagation(self, X):\n",
        "        \"\"\"\n",
        "        Implement forward propagation when training neural network\n",
        "        \"\"\"\n",
        "        A1 = self.Con1D.forward(X)\n",
        "        self.A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
        "        self.Z1 = self.activation1.forward(self.A1)\n",
        "        self.A2 = self.FC2.forward(self.Z1)\n",
        "        self.Z2 = self.activation2.forward(self.A2)\n",
        "        self.A3 = self.FC3.forward(self.Z2)\n",
        "        self.Z3 = self.activation3.forward(self.A3)\n",
        "\n",
        "    def backward_propagation(self, X, y):\n",
        "        \"\"\"\n",
        "        Implement backward propagation \n",
        "        \"\"\"\n",
        "        dA3 = self.activation3.backward(y) # The cross entropy error and softmax are matched.\n",
        "        dZ2 = self.FC3.backward(dA3)\n",
        "        dA2 = self.activation2.backward(dZ2)\n",
        "        dZ1 = self.FC2.backward(dA2)\n",
        "        dA1 = self.activation1.backward(dZ1)\n",
        "        dA1 = dA1[:, np.newaxis]\n",
        "        dZ0 = self.Con1D.backward(dA1)\n",
        " \n",
        "\n",
        "\n",
        "    def _cross_entropy(self, y, Z):\n",
        "        return -np.sum(y*np.log(Z)) / len(y)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            The following form of ndarray, shape (n_samples, 1)\n",
        "            Estimated result\n",
        "        \"\"\"\n",
        "        self.forward_propagation(X)\n",
        "        return np.argmax(self.Z3,axis=1)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnWfYXcYqMCp",
        "outputId": "3dc30a90-c91c-4001-f097-52a4d9310f45"
      },
      "source": [
        "model = ScratchCNNClassifier(verbose=True)\n",
        "model.fit(X_training,y_training, X_val, y_val)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n",
            "Training_loss: 0.14756797059721863\n",
            "Validation loss: 0.15660655347662342\n",
            "Epoch 1:\n",
            "Training_loss: 0.10030125771699551\n",
            "Validation loss: 0.12255594994263476\n",
            "Epoch 2:\n",
            "Training_loss: 0.08477536582851722\n",
            "Validation loss: 0.11885066012967202\n",
            "Epoch 3:\n",
            "Training_loss: 0.07374055782868626\n",
            "Validation loss: 0.11750401038569529\n",
            "Epoch 4:\n",
            "Training_loss: 0.059041328104462075\n",
            "Validation loss: 0.1109224404467596\n",
            "Epoch 5:\n",
            "Training_loss: 0.04705336350288988\n",
            "Validation loss: 0.10526581648741923\n",
            "Epoch 6:\n",
            "Training_loss: 0.03917914268363305\n",
            "Validation loss: 0.1036922321907295\n",
            "Epoch 7:\n",
            "Training_loss: 0.03543416039515492\n",
            "Validation loss: 0.10416897943203943\n",
            "Epoch 8:\n",
            "Training_loss: 0.025351647144638997\n",
            "Validation loss: 0.09584269553024603\n",
            "Epoch 9:\n",
            "Training_loss: 0.018465606511038908\n",
            "Validation loss: 0.0900642028036753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPF1oWTWqbC-",
        "outputId": "39022852-fe86-4665-f70a-041ffdfe75d8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "training_pred_default = model.predict(X=X_train)\n",
        "test_pred_default = model.predict(X=X_test)\n",
        "\n",
        "print(\"Accuracy score for training set with default model: {}\".format(accuracy_score(y_true=y_train, y_pred=training_pred_default)))\n",
        "print(\"Accuracy score for testing set with default model: {}\".format(accuracy_score(y_true=y_test,y_pred=test_pred_default)))\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for training set with default model: 0.9905833333333334\n",
            "Accuracy score for testing set with default model: 0.9726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q248OJrbvCUo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}