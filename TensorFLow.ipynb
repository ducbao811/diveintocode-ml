{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFLow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUWqjGQB6KACXnJmyiV4dd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ducbao811/diveintocode-ml/blob/master/TensorFLow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1]"
      ],
      "metadata": {
        "id": "q2xSEZu8xmSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From what I have learnt so far in the course, below is what we need to do to implement deep learning:\n",
        "\n",
        "* Weight and bias initialization\n",
        "* Deciding what is the activation function for each layer and number of nodes in each layer\n",
        "* Deciding batch size when perform forward/backward propagation\n",
        "* Other parameters like epoch loop, learning rate,...\n",
        "\n"
      ],
      "metadata": {
        "id": "BXpk1_tUzCyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 2]"
      ],
      "metadata": {
        "id": "fbO-YaRiy_kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Binary classification of Iris dataset using neural network implemented in TensorFlow\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.test.gpu_device_name()\n",
        "tf.disable_eager_execution() \n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "#Condition extraction from data frame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Convert label to number\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRT-2NjfAkpH",
        "outputId": "f520c35e-98ce-4f1e-b936-555251fe391d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 2.5685, val_loss : 19.7636, acc : 0.375\n",
            "Epoch 1, loss : 0.4319, val_loss : 2.8135, acc : 0.750\n",
            "Epoch 2, loss : 0.5106, val_loss : 0.7691, acc : 0.812\n",
            "Epoch 3, loss : 0.0792, val_loss : 2.5278, acc : 0.500\n",
            "Epoch 4, loss : 0.1210, val_loss : 0.6748, acc : 0.812\n",
            "Epoch 5, loss : 0.0096, val_loss : 0.1199, acc : 0.938\n",
            "Epoch 6, loss : 0.0215, val_loss : 0.0272, acc : 1.000\n",
            "Epoch 7, loss : 0.0045, val_loss : 0.0282, acc : 1.000\n",
            "Epoch 8, loss : 0.0059, val_loss : 0.0259, acc : 1.000\n",
            "Epoch 9, loss : 0.0039, val_loss : 0.0121, acc : 1.000\n",
            "Epoch 10, loss : 0.0024, val_loss : 0.0215, acc : 1.000\n",
            "Epoch 11, loss : 0.0028, val_loss : 0.0118, acc : 1.000\n",
            "Epoch 12, loss : 0.0022, val_loss : 0.0104, acc : 1.000\n",
            "Epoch 13, loss : 0.0020, val_loss : 0.0114, acc : 1.000\n",
            "Epoch 14, loss : 0.0018, val_loss : 0.0123, acc : 1.000\n",
            "Epoch 15, loss : 0.0018, val_loss : 0.0107, acc : 1.000\n",
            "Epoch 16, loss : 0.0017, val_loss : 0.0102, acc : 1.000\n",
            "Epoch 17, loss : 0.0016, val_loss : 0.0105, acc : 1.000\n",
            "Epoch 18, loss : 0.0016, val_loss : 0.0103, acc : 1.000\n",
            "Epoch 19, loss : 0.0015, val_loss : 0.0098, acc : 1.000\n",
            "Epoch 20, loss : 0.0015, val_loss : 0.0097, acc : 1.000\n",
            "Epoch 21, loss : 0.0014, val_loss : 0.0096, acc : 1.000\n",
            "Epoch 22, loss : 0.0014, val_loss : 0.0095, acc : 1.000\n",
            "Epoch 23, loss : 0.0013, val_loss : 0.0093, acc : 1.000\n",
            "Epoch 24, loss : 0.0013, val_loss : 0.0092, acc : 1.000\n",
            "Epoch 25, loss : 0.0012, val_loss : 0.0091, acc : 1.000\n",
            "Epoch 26, loss : 0.0012, val_loss : 0.0090, acc : 1.000\n",
            "Epoch 27, loss : 0.0012, val_loss : 0.0088, acc : 1.000\n",
            "Epoch 28, loss : 0.0011, val_loss : 0.0087, acc : 1.000\n",
            "Epoch 29, loss : 0.0011, val_loss : 0.0085, acc : 1.000\n",
            "Epoch 30, loss : 0.0011, val_loss : 0.0084, acc : 1.000\n",
            "Epoch 31, loss : 0.0010, val_loss : 0.0083, acc : 1.000\n",
            "Epoch 32, loss : 0.0010, val_loss : 0.0082, acc : 1.000\n",
            "Epoch 33, loss : 0.0010, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 34, loss : 0.0009, val_loss : 0.0080, acc : 1.000\n",
            "Epoch 35, loss : 0.0009, val_loss : 0.0079, acc : 1.000\n",
            "Epoch 36, loss : 0.0009, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 37, loss : 0.0009, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 38, loss : 0.0009, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 39, loss : 0.0008, val_loss : 0.0076, acc : 1.000\n",
            "Epoch 40, loss : 0.0008, val_loss : 0.0076, acc : 1.000\n",
            "Epoch 41, loss : 0.0008, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 42, loss : 0.0008, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 43, loss : 0.0008, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 44, loss : 0.0007, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 45, loss : 0.0007, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 46, loss : 0.0007, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 47, loss : 0.0007, val_loss : 0.0074, acc : 1.000\n",
            "Epoch 48, loss : 0.0007, val_loss : 0.0074, acc : 1.000\n",
            "Epoch 49, loss : 0.0007, val_loss : 0.0074, acc : 1.000\n",
            "Epoch 50, loss : 0.0007, val_loss : 0.0074, acc : 1.000\n",
            "Epoch 51, loss : 0.0006, val_loss : 0.0074, acc : 1.000\n",
            "Epoch 52, loss : 0.0006, val_loss : 0.0074, acc : 1.000\n",
            "Epoch 53, loss : 0.0006, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 54, loss : 0.0006, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 55, loss : 0.0006, val_loss : 0.0075, acc : 1.000\n",
            "Epoch 56, loss : 0.0006, val_loss : 0.0076, acc : 1.000\n",
            "Epoch 57, loss : 0.0006, val_loss : 0.0076, acc : 1.000\n",
            "Epoch 58, loss : 0.0006, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 59, loss : 0.0006, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 60, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 61, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 62, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 63, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 64, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 65, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 66, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 67, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 68, loss : 0.0005, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 69, loss : 0.0005, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 70, loss : 0.0005, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 71, loss : 0.0005, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 72, loss : 0.0005, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 73, loss : 0.0005, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 74, loss : 0.0004, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 75, loss : 0.0004, val_loss : 0.0078, acc : 1.000\n",
            "Epoch 76, loss : 0.0004, val_loss : 0.0079, acc : 1.000\n",
            "Epoch 77, loss : 0.0004, val_loss : 0.0079, acc : 1.000\n",
            "Epoch 78, loss : 0.0004, val_loss : 0.0079, acc : 1.000\n",
            "Epoch 79, loss : 0.0004, val_loss : 0.0080, acc : 1.000\n",
            "Epoch 80, loss : 0.0004, val_loss : 0.0080, acc : 1.000\n",
            "Epoch 81, loss : 0.0004, val_loss : 0.0080, acc : 1.000\n",
            "Epoch 82, loss : 0.0004, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 83, loss : 0.0004, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 84, loss : 0.0004, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 85, loss : 0.0004, val_loss : 0.0082, acc : 1.000\n",
            "Epoch 86, loss : 0.0004, val_loss : 0.0082, acc : 1.000\n",
            "Epoch 87, loss : 0.0004, val_loss : 0.0082, acc : 1.000\n",
            "Epoch 88, loss : 0.0004, val_loss : 0.0083, acc : 1.000\n",
            "Epoch 89, loss : 0.0004, val_loss : 0.0083, acc : 1.000\n",
            "Epoch 90, loss : 0.0004, val_loss : 0.0083, acc : 1.000\n",
            "Epoch 91, loss : 0.0004, val_loss : 0.0083, acc : 1.000\n",
            "Epoch 92, loss : 0.0004, val_loss : 0.0084, acc : 1.000\n",
            "Epoch 93, loss : 0.0004, val_loss : 0.0084, acc : 1.000\n",
            "Epoch 94, loss : 0.0004, val_loss : 0.0084, acc : 1.000\n",
            "Epoch 95, loss : 0.0004, val_loss : 0.0084, acc : 1.000\n",
            "Epoch 96, loss : 0.0004, val_loss : 0.0084, acc : 1.000\n",
            "Epoch 97, loss : 0.0003, val_loss : 0.0083, acc : 1.000\n",
            "Epoch 98, loss : 0.0003, val_loss : 0.0083, acc : 1.000\n",
            "Epoch 99, loss : 0.0003, val_loss : 0.0082, acc : 1.000\n",
            "test_acc : 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Weight and bias are initialized with\n",
        "```\n",
        "tf.random_normal()\n",
        "```\n",
        "*   Adam Optimizer is used for gradient descent \n",
        "\n"
      ],
      "metadata": {
        "id": "P1N8xGjjCi23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 3]"
      ],
      "metadata": {
        "id": "MJrLe9ZaB2IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.test.gpu_device_name()\n",
        "tf.disable_eager_execution()"
      ],
      "metadata": {
        "id": "lSfsElxi25IQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "y = df[\"Species\"].values\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
        "\n",
        "# Covert label y\n",
        "y = OneHotEncoder().fit_transform(y[:,np.newaxis]).toarray()"
      ],
      "metadata": {
        "id": "0TFGtk9_-w5s"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "5AAWdkRmBPce"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3"
      ],
      "metadata": {
        "id": "E_2_PKtyDMaE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "by9gB9TRFikT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.arg_max(Y, 1), tf.arg_max(logits, 1))\n",
        "\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "pLNxsARnFn2G"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdsTO4JOHi2c",
        "outputId": "22a9f0fd-0628-4611-da80-d3818e2c350c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.1547, val_loss : 60.1149, acc : 0.292\n",
            "Epoch 1, loss : 5.0318, val_loss : 41.7341, acc : 0.375\n",
            "Epoch 2, loss : 3.7180, val_loss : 34.6120, acc : 0.375\n",
            "Epoch 3, loss : 3.1345, val_loss : 28.0894, acc : 0.375\n",
            "Epoch 4, loss : 2.4825, val_loss : 22.4513, acc : 0.583\n",
            "Epoch 5, loss : 1.9380, val_loss : 16.9703, acc : 0.583\n",
            "Epoch 6, loss : 1.4045, val_loss : 11.6546, acc : 0.583\n",
            "Epoch 7, loss : 0.9012, val_loss : 6.8562, acc : 0.625\n",
            "Epoch 8, loss : 0.4005, val_loss : 2.1905, acc : 0.917\n",
            "Epoch 9, loss : 0.0840, val_loss : 0.5242, acc : 0.917\n",
            "Epoch 10, loss : 0.0353, val_loss : 0.3331, acc : 0.917\n",
            "Epoch 11, loss : 0.0244, val_loss : 0.3380, acc : 0.917\n",
            "Epoch 12, loss : 0.0199, val_loss : 0.3053, acc : 0.917\n",
            "Epoch 13, loss : 0.0175, val_loss : 0.3677, acc : 0.917\n",
            "Epoch 14, loss : 0.0157, val_loss : 0.3124, acc : 0.917\n",
            "Epoch 15, loss : 0.0140, val_loss : 0.3236, acc : 0.917\n",
            "Epoch 16, loss : 0.0123, val_loss : 0.3141, acc : 0.917\n",
            "Epoch 17, loss : 0.0108, val_loss : 0.3142, acc : 0.917\n",
            "Epoch 18, loss : 0.0092, val_loss : 0.3100, acc : 0.917\n",
            "Epoch 19, loss : 0.0076, val_loss : 0.3058, acc : 0.917\n",
            "Epoch 20, loss : 0.0061, val_loss : 0.3027, acc : 0.917\n",
            "Epoch 21, loss : 0.0051, val_loss : 0.2961, acc : 0.917\n",
            "Epoch 22, loss : 0.0047, val_loss : 0.2908, acc : 0.917\n",
            "Epoch 23, loss : 0.0045, val_loss : 0.2854, acc : 0.917\n",
            "Epoch 24, loss : 0.0043, val_loss : 0.2803, acc : 0.917\n",
            "Epoch 25, loss : 0.0041, val_loss : 0.2760, acc : 0.917\n",
            "Epoch 26, loss : 0.0040, val_loss : 0.2720, acc : 0.917\n",
            "Epoch 27, loss : 0.0039, val_loss : 0.2713, acc : 0.917\n",
            "Epoch 28, loss : 0.0038, val_loss : 0.2696, acc : 0.917\n",
            "Epoch 29, loss : 0.0037, val_loss : 0.2690, acc : 0.917\n",
            "Epoch 30, loss : 0.0037, val_loss : 0.2679, acc : 0.917\n",
            "Epoch 31, loss : 0.0036, val_loss : 0.2673, acc : 0.917\n",
            "Epoch 32, loss : 0.0035, val_loss : 0.2660, acc : 0.917\n",
            "Epoch 33, loss : 0.0034, val_loss : 0.2649, acc : 0.917\n",
            "Epoch 34, loss : 0.0034, val_loss : 0.2637, acc : 0.917\n",
            "Epoch 35, loss : 0.0033, val_loss : 0.2626, acc : 0.917\n",
            "Epoch 36, loss : 0.0032, val_loss : 0.2615, acc : 0.917\n",
            "Epoch 37, loss : 0.0032, val_loss : 0.2606, acc : 0.917\n",
            "Epoch 38, loss : 0.0031, val_loss : 0.2595, acc : 0.917\n",
            "Epoch 39, loss : 0.0031, val_loss : 0.2584, acc : 0.917\n",
            "Epoch 40, loss : 0.0030, val_loss : 0.2569, acc : 0.917\n",
            "Epoch 41, loss : 0.0030, val_loss : 0.2556, acc : 0.917\n",
            "Epoch 42, loss : 0.0029, val_loss : 0.2538, acc : 0.917\n",
            "Epoch 43, loss : 0.0028, val_loss : 0.2513, acc : 0.917\n",
            "Epoch 44, loss : 0.0028, val_loss : 0.2507, acc : 0.917\n",
            "Epoch 45, loss : 0.0027, val_loss : 0.2503, acc : 0.917\n",
            "Epoch 46, loss : 0.0027, val_loss : 0.2495, acc : 0.917\n",
            "Epoch 47, loss : 0.0027, val_loss : 0.2488, acc : 0.917\n",
            "Epoch 48, loss : 0.0026, val_loss : 0.2474, acc : 0.917\n",
            "Epoch 49, loss : 0.0026, val_loss : 0.2464, acc : 0.917\n",
            "Epoch 50, loss : 0.0025, val_loss : 0.2448, acc : 0.917\n",
            "Epoch 51, loss : 0.0025, val_loss : 0.2439, acc : 0.917\n",
            "Epoch 52, loss : 0.0025, val_loss : 0.2422, acc : 0.917\n",
            "Epoch 53, loss : 0.0024, val_loss : 0.2414, acc : 0.917\n",
            "Epoch 54, loss : 0.0024, val_loss : 0.2398, acc : 0.917\n",
            "Epoch 55, loss : 0.0024, val_loss : 0.2391, acc : 0.917\n",
            "Epoch 56, loss : 0.0023, val_loss : 0.2376, acc : 0.917\n",
            "Epoch 57, loss : 0.0023, val_loss : 0.2370, acc : 0.917\n",
            "Epoch 58, loss : 0.0023, val_loss : 0.2357, acc : 0.917\n",
            "Epoch 59, loss : 0.0022, val_loss : 0.2357, acc : 0.917\n",
            "Epoch 60, loss : 0.0022, val_loss : 0.2348, acc : 0.917\n",
            "Epoch 61, loss : 0.0022, val_loss : 0.2350, acc : 0.917\n",
            "Epoch 62, loss : 0.0021, val_loss : 0.2339, acc : 0.917\n",
            "Epoch 63, loss : 0.0021, val_loss : 0.2376, acc : 0.917\n",
            "Epoch 64, loss : 0.0021, val_loss : 0.2387, acc : 0.917\n",
            "Epoch 65, loss : 0.0021, val_loss : 0.2343, acc : 0.917\n",
            "Epoch 66, loss : 0.0020, val_loss : 0.2322, acc : 0.917\n",
            "Epoch 67, loss : 0.0020, val_loss : 0.2300, acc : 0.917\n",
            "Epoch 68, loss : 0.0020, val_loss : 0.2338, acc : 0.917\n",
            "Epoch 69, loss : 0.0020, val_loss : 0.2352, acc : 0.917\n",
            "Epoch 70, loss : 0.0020, val_loss : 0.2317, acc : 0.917\n",
            "Epoch 71, loss : 0.0019, val_loss : 0.2299, acc : 0.917\n",
            "Epoch 72, loss : 0.0019, val_loss : 0.2271, acc : 0.917\n",
            "Epoch 73, loss : 0.0019, val_loss : 0.2310, acc : 0.917\n",
            "Epoch 74, loss : 0.0019, val_loss : 0.2319, acc : 0.917\n",
            "Epoch 75, loss : 0.0019, val_loss : 0.2289, acc : 0.917\n",
            "Epoch 76, loss : 0.0018, val_loss : 0.2267, acc : 0.917\n",
            "Epoch 77, loss : 0.0018, val_loss : 0.2246, acc : 0.917\n",
            "Epoch 78, loss : 0.0018, val_loss : 0.2336, acc : 0.917\n",
            "Epoch 79, loss : 0.0018, val_loss : 0.2374, acc : 0.917\n",
            "Epoch 80, loss : 0.0018, val_loss : 0.2337, acc : 0.917\n",
            "Epoch 81, loss : 0.0017, val_loss : 0.2313, acc : 0.917\n",
            "Epoch 82, loss : 0.0017, val_loss : 0.2309, acc : 0.917\n",
            "Epoch 83, loss : 0.0017, val_loss : 0.2342, acc : 0.917\n",
            "Epoch 84, loss : 0.0017, val_loss : 0.2274, acc : 0.917\n",
            "Epoch 85, loss : 0.0017, val_loss : 0.2286, acc : 0.917\n",
            "Epoch 86, loss : 0.0017, val_loss : 0.2255, acc : 0.917\n",
            "Epoch 87, loss : 0.0017, val_loss : 0.2324, acc : 0.917\n",
            "Epoch 88, loss : 0.0016, val_loss : 0.2228, acc : 0.917\n",
            "Epoch 89, loss : 0.0016, val_loss : 0.2338, acc : 0.917\n",
            "Epoch 90, loss : 0.0016, val_loss : 0.2297, acc : 0.917\n",
            "Epoch 91, loss : 0.0016, val_loss : 0.2396, acc : 0.917\n",
            "Epoch 92, loss : 0.0016, val_loss : 0.2266, acc : 0.917\n",
            "Epoch 93, loss : 0.0016, val_loss : 0.2338, acc : 0.917\n",
            "Epoch 94, loss : 0.0016, val_loss : 0.2237, acc : 0.917\n",
            "Epoch 95, loss : 0.0016, val_loss : 0.2360, acc : 0.917\n",
            "Epoch 96, loss : 0.0015, val_loss : 0.2195, acc : 0.917\n",
            "Epoch 97, loss : 0.0015, val_loss : 0.2319, acc : 0.917\n",
            "Epoch 98, loss : 0.0015, val_loss : 0.2159, acc : 0.917\n",
            "Epoch 99, loss : 0.0015, val_loss : 0.2413, acc : 0.917\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 4] Creating a model of House Prices"
      ],
      "metadata": {
        "id": "s_pRiTwTJEtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "caYQPMCMHqSX",
        "outputId": "1796097e-6e1d-4753-96aa-b3e74d5c970c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-325f49b7-2a6f-4d17-9d1c-0756fe4447b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>...</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>856</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>548</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "      <td>1262</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>460</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>434</td>\n",
              "      <td>920</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>608</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>540</td>\n",
              "      <td>756</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3</td>\n",
              "      <td>642</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "      <td>1145</td>\n",
              "      <td>GasA</td>\n",
              "      <td>...</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3</td>\n",
              "      <td>836</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-325f49b7-2a6f-4d17-9d1c-0756fe4447b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-325f49b7-2a6f-4d17-9d1c-0756fe4447b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-325f49b7-2a6f-4d17-9d1c-0756fe4447b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice\n",
              "0   1          60       RL  ...        WD         Normal    208500\n",
              "1   2          20       RL  ...        WD         Normal    181500\n",
              "2   3          60       RL  ...        WD         Normal    223500\n",
              "3   4          70       RL  ...        WD        Abnorml    140000\n",
              "4   5          60       RL  ...        WD         Normal    250000\n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[[\"GrLivArea\",\"YearBuilt\"]].values\n",
        "y = df[['SalePrice']].values\n",
        "\n",
        "# Since the target variable distribution is positive skewed, we need to log-transform its\n",
        "y = np.log1p(y)"
      ],
      "metadata": {
        "id": "ZMq6hWMUKX8W"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "IVXBcds4K8p0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 30\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1"
      ],
      "metadata": {
        "id": "2Bshl9fCK_R2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "sXdPrFOvOw1E"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = example_net(X)\n",
        "\n",
        "# Objective function MSE\n",
        "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "9oHDtnCfPGed"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        loss_list.append(total_loss)\n",
        "        val_loss_list.append(val_loss)\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, total_loss, val_loss))\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "4mV64ih4Q4bg",
        "outputId": "a718cea6-c4b4-4b24-a32c-567cffbe3f3e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 27049395.7998, val_loss : 11196228.0000\n",
            "Epoch 1, loss : 434400.5127, val_loss : 1198504.8750\n",
            "Epoch 2, loss : 55163.8015, val_loss : 225871.1094\n",
            "Epoch 3, loss : 22207.8463, val_loss : 120487.1094\n",
            "Epoch 4, loss : 13422.5239, val_loss : 78935.0859\n",
            "Epoch 5, loss : 9215.1863, val_loss : 56751.0977\n",
            "Epoch 6, loss : 6724.6721, val_loss : 43470.7422\n",
            "Epoch 7, loss : 5130.3215, val_loss : 33162.2461\n",
            "Epoch 8, loss : 4012.8192, val_loss : 25609.6387\n",
            "Epoch 9, loss : 3258.5387, val_loss : 20402.6211\n",
            "Epoch 10, loss : 2712.2869, val_loss : 16983.4844\n",
            "Epoch 11, loss : 2317.8834, val_loss : 14474.1543\n",
            "Epoch 12, loss : 2020.4940, val_loss : 12693.2656\n",
            "Epoch 13, loss : 1783.9029, val_loss : 11301.4561\n",
            "Epoch 14, loss : 1585.0084, val_loss : 10578.5830\n",
            "Epoch 15, loss : 1427.0282, val_loss : 10403.4395\n",
            "Epoch 16, loss : 1312.1517, val_loss : 10750.7734\n",
            "Epoch 17, loss : 1218.9380, val_loss : 11414.8604\n",
            "Epoch 18, loss : 1133.6396, val_loss : 11634.6787\n",
            "Epoch 19, loss : 1049.2273, val_loss : 11180.0996\n",
            "Epoch 20, loss : 962.2123, val_loss : 10220.0596\n",
            "Epoch 21, loss : 884.6625, val_loss : 9056.3721\n",
            "Epoch 22, loss : 814.4111, val_loss : 7881.6616\n",
            "Epoch 23, loss : 751.0446, val_loss : 6804.3662\n",
            "Epoch 24, loss : 694.1155, val_loss : 5915.8584\n",
            "Epoch 25, loss : 641.2681, val_loss : 5319.7480\n",
            "Epoch 26, loss : 595.3375, val_loss : 4941.7778\n",
            "Epoch 27, loss : 557.3334, val_loss : 4796.5864\n",
            "Epoch 28, loss : 525.7236, val_loss : 4887.4941\n",
            "Epoch 29, loss : 497.0918, val_loss : 5234.5122\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 5: Creating a MNIST model]"
      ],
      "metadata": {
        "id": "KGDFte1mWsZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_val = enc.transform(y_val[:, np.newaxis])\n",
        "y_test = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCyhbvUSRQUQ",
        "outputId": "65b964c1-0cb0-4ecf-8125-9c6901d9aa93"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(48000, 784)\n",
            "(48000,)\n",
            "(12000, 784)\n",
            "(12000,)\n",
            "(48000, 10)\n",
            "(12000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter settings\n",
        "learning_rate = 0.01\n",
        "batch_size = 100\n",
        "num_epochs = 20\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10"
      ],
      "metadata": {
        "id": "wKltiHicX84d"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "jC31Fz44YzF4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.arg_max(Y, 1), tf.arg_max(logits, 1))\n",
        "\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "NdD758s4Y1nO"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICVf1hEWbFzL",
        "outputId": "5fff6a0c-f810-420f-8c3a-62235443c060"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.7702, val_loss : 0.5031, acc : 0.710, val_acc : 0.798\n",
            "Epoch 1, loss : 0.2932, val_loss : 0.2298, acc : 0.750, val_acc : 0.785\n",
            "Epoch 2, loss : 0.1819, val_loss : 0.1551, acc : 0.770, val_acc : 0.814\n",
            "Epoch 3, loss : 0.1655, val_loss : 0.1256, acc : 0.750, val_acc : 0.836\n",
            "Epoch 4, loss : 0.1356, val_loss : 0.1079, acc : 0.820, val_acc : 0.856\n",
            "Epoch 5, loss : 0.0965, val_loss : 0.0994, acc : 0.850, val_acc : 0.869\n",
            "Epoch 6, loss : 0.0699, val_loss : 0.0904, acc : 0.880, val_acc : 0.873\n",
            "Epoch 7, loss : 0.0562, val_loss : 0.0849, acc : 0.900, val_acc : 0.885\n",
            "Epoch 8, loss : 0.0492, val_loss : 0.0779, acc : 0.920, val_acc : 0.896\n",
            "Epoch 9, loss : 0.0442, val_loss : 0.0765, acc : 0.930, val_acc : 0.909\n",
            "Epoch 10, loss : 0.0458, val_loss : 0.0708, acc : 0.930, val_acc : 0.913\n",
            "Epoch 11, loss : 0.0390, val_loss : 0.0703, acc : 0.940, val_acc : 0.911\n",
            "Epoch 12, loss : 0.0420, val_loss : 0.0658, acc : 0.930, val_acc : 0.914\n",
            "Epoch 13, loss : 0.0547, val_loss : 0.0642, acc : 0.940, val_acc : 0.922\n",
            "Epoch 14, loss : 0.0571, val_loss : 0.0646, acc : 0.910, val_acc : 0.919\n",
            "Epoch 15, loss : 0.0450, val_loss : 0.0584, acc : 0.930, val_acc : 0.925\n",
            "Epoch 16, loss : 0.0493, val_loss : 0.0653, acc : 0.920, val_acc : 0.922\n",
            "Epoch 17, loss : 0.0378, val_loss : 0.0571, acc : 0.940, val_acc : 0.930\n",
            "Epoch 18, loss : 0.0515, val_loss : 0.0532, acc : 0.900, val_acc : 0.929\n",
            "Epoch 19, loss : 0.0391, val_loss : 0.0531, acc : 0.910, val_acc : 0.933\n",
            "test_acc : 0.936\n"
          ]
        }
      ]
    }
  ]
}